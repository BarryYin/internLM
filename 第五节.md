
# LMDeploy 的量化和部署

今天学习的主题是LMDeploy 的量化和部署。

视频地址：https://www.bilibili.com/video/BV1iW4y1A77P

文档地址：https://github.com/InternLM/tutorial/blob/vansin-patch-4/lmdeploy/lmdeploy.md

## 预习知识

这节课需要有些预习知识：

1.什么是量化部署

模型量化是深度学习的一项基本技术，旨在解决与模型大小、推理速度和内存效率相关的关键挑战。它通过将模型权重从高精度浮点表示（通常为 32 位 (FP32)）转换为低精度浮点 (FP) 或整数 (INT) 格式（例如 16 位或 8 位）来实现此目的。

量化的好处是双重的。首先，它显着减少了模型的内存占用并提高了推理速度，而不会导致准确性大幅下降。其次，它通过降低内存带宽要求和提高缓存利用率来优化模型性能。

在深度神经网络中，INT8 表示通常被通俗地称为“量化”，但也可以使用 UINT8 和 INT16 等其他格式，具体取决于硬件架构。不同的模型需要不同的量化方法，通常需要先验知识和细致的微调来平衡精度和模型尺寸的减小。

量化带来了挑战，特别是对于 INT8 等低精度整数格式，因为它们的动态范围有限。将 FP32 的广阔动态范围压缩为 INT8 的 255 个值可能会导致精度损失。为了缓解这一挑战，每通道或每层缩放会调整权重和激活张量的比例和零点值，以更好地适应量化格式。

此外，量化感知训练模拟模型训练期间的量化过程，使模型能够优雅地适应较低的精度。挤压或范围估计是该过程的一个重要方面，通过校准实现。

从本质上讲，模型量化对于部署高效的人工智能模型、在准确性和资源效率之间取得微妙的平衡是必不可少的，特别是在计算资源有限的边缘设备上。

2、模型量化技术

量化神经网络有多种算法和方法。一些标准量化技术包括：

![Alt text](src\6-image-15.png)

权重量化 涉及将模型的权重量化为较低精度的值（例如，8 位整数）。权重量化可以显着减少模型的内存占用。
激活量化：除了量化权重之外，还可以在推理过程中量化激活。这进一步减少了计算要求和内存使用。
动态量化：动态量化不使用固定的量化比例，而是允许在推理过程中动态缩放量化范围，有助于减轻准确性的损失。
量化感知训练（QAT）：如前所述，QAT 是一种结合量化约束的训练方法，使模型能够学习使用较低精度的数据进行操作。
混合精度量化：该技术结合了权重和激活的不同精度量化，优化了准确性和效率。
带校准的训练后量化：在训练后量化中，校准用于确定权重和激活的量化范围，以最大限度地减少准确性的损失。
综上所述，训练后量化和量化感知训练（QAT）的选择取决于具体的部署需求以及模型性能和效率之间的平衡。PTQ 提供了一种更直接的方法来减小模型大小。尽管如此，由于原始全精度模型与其量化对应模型之间固有的不匹配，它可能会遭受精度损失。另一方面，QAT 将量化约束直接集成到训练过程中，确保模型从一开始就学会如何有效地使用较低精度的数据进行操作。

## LMDeploy 大模型部署实践

我们通过课程学习，可以看到LMDeploy 要解决的就是大模型部署的一些痛点，比如轻量化的部署，和支持多种部署方式


![Alt text](src\6-image.png)

![Alt text](src\6-image-1.png)

![Alt text](src\6-image-2.png)

![Alt text](src\6-image-3.png)

![Alt text](src\6-image-4.png)

## LMDeploy 三大核心功能


![Alt text](src\6-image-5.png)

![Alt text](src\6-image-6.png)
![Alt text](src\6-image-7.png)

## LMDeploy 动手实践

动手实践部分直接看作业

