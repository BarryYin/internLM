# 第六节 大模型评测

## 预习
大模型的常用评估指标
- EM exact match
em 表示预测值和答案是否完全一样。
- F1
分别计算准确率和召回率， F1 是准确率和召回率的调和平均数。
- Accuracy 和 Accuracy norm
 accuracy 代表正确的（正确的、召回的部分 + 正确的、没召回的部分）比例。适合于离散的结果、分类任务，比如选择题。
- Perplexity 困惑度
困惑度（perplexity）的基本思想是：模型对于一个测试集中的句子，计算这个句子中词组合出现的概率，概率越高，困惑度越低，模型性能就证明是越好。


## 关于大语言模型评测的三个问题

![Alt text](src\7-image-1.png)

- 为什么需要评测大语言模型？
不同的模型质量如何，表现如何？
不同的使用者，对于大模型评测有不同的需求。

- 我们需要测什么
![Alt text](src\7-image-2.png)

主要评测基础模型 + 对话模型


### 客观评测
问答题
多选题
判断题
针对不同题目的答案进行客观评分
### 主观评测
非客观题，对两个模型之间的表现进行比较
由人类进行评价和模型评价，在同维度评测下A和B之间的相对分

![Alt text](src\7-image-3.png)

### 提示词工程
通过提示词指令微调后，看同一个模型的前后表现差异

### 主流评测框架 

不同机构的评测内容和评测方法不尽相同，需要根据自己的需求进行选择。

![Alt text](src\7-image-4.png)
### OpenCompass

OpenCompass 收集了100万+评测集，和50万+评测题目

![Alt text](src\7-image-5.png)

OpenCompass的组成结构，但看这个框架就感觉很全面

![Alt text](src\7-image-6.png)

为了更好地激发出模型在题目测试领域的能力，并引导模型按照一定的模板输出答案，OpenCompass采用提示词工程 （prompt engineering）和语境学习（in-context learning）进行客观评测。

在客观评测的具体实践中，我们通常采用下列两种方式进行模型输出结果的评测：

判别式评测：该评测方式基于将问题与候选答案组合在一起，计算模型在所有组合上的困惑度（perplexity），并选择困惑度最小的答案作为模型的最终输出。例如，若模型在 问题? 答案1 上的困惑度为 0.1，在 问题? 答案2 上的困惑度为 0.2，最终我们会选择 答案1 作为模型的输出。

生成式评测：该评测方式主要用于生成类任务，如语言翻译、程序生成、逻辑分析题等。具体实践时，使用问题作为模型的原始输入，并留白答案区域待模型进行后续补全。我们通常还需要对其输出进行后处理，以保证输出满足数据集的要求。



### 当前大模型评测领域的挑战

我感觉最大的挑战是大家都拿评测集来做训练

![Alt text](src\7-image-7.png)

